import requests
import json
import time
import random
import logging
from collections import Counter
import nltk
from nltk.corpus import stopwords
from urllib.parse import quote

# === üî• –ù–∞—Å—Ç—Ä–æ–π–∫–∏ API ===
GOOGLE_API_KEY = "AIzaSyC3osOG8zTc7WLbyNYAQTTAvuRLb2tiS8E"
GOOGLE_CX_KEY = "2518e0c19fbe043a0"

# === üî• –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(message)s")

# === üî• –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π ===
try:
    with open("knowledge_base.json", "r", encoding="utf-8") as f:
        knowledge_base = json.load(f)
except FileNotFoundError:
    knowledge_base = []

# === üî• –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º ===
try:
    with open("explored_topics.json", "r", encoding="utf-8") as f:
        explored_topics = json.load(f)
except FileNotFoundError:
    explored_topics = []

# === üî• –ó–∞–≥—Ä—É–∑–∫–∞ NLTK ===
nltk.download("stopwords")
STOPWORDS = set(stopwords.words("russian")) | set(stopwords.words("english"))

# === üî• –ß—ë—Ä–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–º (—á—Ç–æ–±—ã –Ω–µ –±—Ä–∞—Ç—å –º—É—Å–æ—Ä) ===
BLACKLIST_TOPICS = {"jan", "inc.", "–∏–∑–º–µ–Ω–µ–Ω–∏–µ", "google", "error",
                    "info", "finance", "company", "university", "professor", "pixel"}


def google_search(query):
    """–ò—â–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ Google –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    url = f"https://www.googleapis.com/customsearch/v1?q={quote(query)}&key={GOOGLE_API_KEY}&cx={GOOGLE_CX_KEY}"
    try:
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [item["snippet"] for item in data.get("items", [])]
        else:
            return [f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {response.status_code}"]
    except Exception as e:
        return [f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"]


def clean_text(text):
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã"""
    words = text.lower().split()
    return [word for word in words if word not in STOPWORDS and len(word) > 2]


def filter_results(results):
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç –Ω–µ–Ω—É–∂–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    filtered = []
    for res in results:
        if len(res) < 30:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ—Ä–∞–∑—ã –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue
        # –ï—Å–ª–∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å —Ç—Ä–æ–µ—Ç–æ—á–∏—è, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)
        if "..." in res[:10]:
            continue
        filtered.append(res)
    return filtered


def analyze_knowledge():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∑–Ω–∞–Ω–∏—è –∏ –¥–µ–ª–∞–µ—Ç –≤—ã–≤–æ–¥—ã"""
    if len(knowledge_base) < 10:
        return "–ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

    word_count = Counter()
    for item in knowledge_base:
        word_count.update(clean_text(item))

    top_terms = [term for term, count in word_count.most_common(5)
                 if term not in BLACKLIST_TOPICS and term != "..."]

    return f"üìä –í—ã–≤–æ–¥—ã –ê–ø–æ–ª–ª–æ–Ω–∞:\nüîπ –¢–û–ü-5 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤: {top_terms}"


def find_new_topic():
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—É—é —Ç–µ–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ —É–∂–µ –∏–∑—É—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    if not knowledge_base:
        return random.choice(["AGI", "–ö–≤–∞–Ω—Ç–æ–≤—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è", "–ë—É–¥—É—â–µ–µ AI", "Web3", "–ú–∞–∫—Å –ö–æ–Ω–∞—Ç–µ"])

    # **–ò—â–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ç–µ—Ä–º–∏–Ω—ã** ‚Äì –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã!
    word_count = Counter()
    for item in knowledge_base:
        word_count.update(clean_text(item))

    common_terms = [term for term, count in word_count.most_common(20)
                    if term not in explored_topics and term not in BLACKLIST_TOPICS and len(term) > 3]

    if common_terms:
        new_topic = random.choice(common_terms)
        explored_topics.append(new_topic)
        return new_topic
    else:
        return random.choice(["AGI", "Web3", "–ë—É–¥—É—â–µ–µ AI", "–ú–∞–∫—Å –ö–æ–Ω–∞—Ç–µ", "–ò–Ω–Ω–æ–≤–∞—Ü–∏–∏ –≤ —Å–ø–æ—Ä—Ç–µ"])


def should_dive_deeper(query):
    """–†–µ—à–∞–µ—Ç, —Å—Ç–æ–∏—Ç –ª–∏ —É–≥–ª—É–±–ª—è—Ç—å—Å—è –≤ —Ç–µ–º—É"""
    results = google_search(query)
    relevant_results = [res for res in filter_results(
        results) if len(res.split()) > 10]

    if len(relevant_results) > 2:
        logging.info(f"üî¨ –ê–ø–æ–ª–ª–æ–Ω –Ω–∞—à–µ–ª –º–Ω–æ–≥–æ –∏–Ω—Ñ—ã –ø—Ä–æ {query} ‚Äì —É–≥–ª—É–±–ª—è–µ–º—Å—è!")
        return True
    return False


def self_learning():
    """üî• –ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ–µ —Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ –ê–ø–æ–ª–ª–æ–Ω–∞ üî•"""
    global knowledge_base, explored_topics
    iteration = 0

    try:
        while True:
            query = find_new_topic()
            logging.info(f"üîç –ê–ø–æ–ª–ª–æ–Ω –∏–∑—É—á–∞–µ—Ç: {query}")

            # **–î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å**
            results = google_search(query)

            # **–§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –º—É—Å–æ—Ä**
            unique_results = [res for res in filter_results(
                results) if res not in knowledge_base]

            if unique_results:
                knowledge_base.extend(unique_results)
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º 3 –ø–µ—Ä–≤—ã—Ö
                logging.info(f"üìö –ù–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è: {unique_results[:3]}")

            # **–°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –∑–Ω–∞–Ω–∏–π**
            with open("knowledge_base.json", "w", encoding="utf-8") as f:
                json.dump(knowledge_base, f, indent=4, ensure_ascii=False)
                logging.info("üíæ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –æ–±–Ω–æ–≤–ª–µ–Ω–∞!")

            # **–°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã—Ö —Ç–µ–º**
            with open("explored_topics.json", "w", encoding="utf-8") as f:
                json.dump(explored_topics, f, indent=4, ensure_ascii=False)

            # **–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–Ω–∞–Ω–∏—è**
            analysis = analyze_knowledge()
            logging.info(analysis)

            # **–ï—Å–ª–∏ —Ç–µ–º–∞ —Ä–µ–∞–ª—å–Ω–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è ‚Äì –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –µ—ë –∏–∑—É—á–∞—Ç—å**
            if should_dive_deeper(query):
                logging.info(f"üîé –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —É–≥–ª—É–±–ª—è—Ç—å—Å—è –≤ {query}!")
                explored_topics.append(query)  # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ –∏–∑—É—á–µ–Ω–Ω—ã—Ö

            iteration += 1
            time.sleep(60)  # –ñ–¥—ë–º 1 –º–∏–Ω—É—Ç—É –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–∏–º –ø–æ–∏—Å–∫–æ–º

    except KeyboardInterrupt:
        logging.info("‚èπÔ∏è  –ê–ø–æ–ª–ª–æ–Ω –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω. üöÄ")


# === üî• –ó–∞–ø—É—Å–∫ ===
if __name__ == "__main__":
    self_learning()
